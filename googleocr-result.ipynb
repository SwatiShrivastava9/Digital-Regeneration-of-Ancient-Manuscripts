{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2f2252c7-f558-446c-803f-63a31152c575",
    "_uuid": "0886e481-b6b9-4c42-8f7b-5b379ed9a360",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-23T07:30:01.933417Z",
     "iopub.status.busy": "2024-07-23T07:30:01.932997Z",
     "iopub.status.idle": "2024-07-23T07:30:01.939391Z",
     "shell.execute_reply": "2024-07-23T07:30:01.938177Z",
     "shell.execute_reply.started": "2024-07-23T07:30:01.933384Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from importlib.resources import path\n",
    "import os,io\n",
    "from google.cloud import vision\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud.vision_v1 import types\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "62660de5-bec2-4cdc-8c60-c891eba52605",
    "_uuid": "8b536fca-9504-4cf4-a86d-8f1c4e5bf4bc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-23T06:05:43.275907Z",
     "iopub.status.busy": "2024-07-23T06:05:43.273100Z",
     "iopub.status.idle": "2024-07-23T06:05:43.281691Z",
     "shell.execute_reply": "2024-07-23T06:05:43.280368Z",
     "shell.execute_reply.started": "2024-07-23T06:05:43.275844Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/kaggle/input/cloudvisionapi-json/manuscripts-430205-87e76d29241b.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "978b6a27-e90c-4442-8ae0-42e102a1f006",
    "_uuid": "0e630cfe-a8e7-4477-b5fa-240aadeebf69",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-23T06:05:47.395476Z",
     "iopub.status.busy": "2024-07-23T06:05:47.394621Z",
     "iopub.status.idle": "2024-07-23T06:05:47.537278Z",
     "shell.execute_reply": "2024-07-23T06:05:47.536168Z",
     "shell.execute_reply.started": "2024-07-23T06:05:47.395442Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "client = vision.ImageAnnotatorClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "309095b9-1a0d-496f-9a64-7be2a8c74b4a",
    "_uuid": "d36fbe2f-f9bd-4b78-b7d5-e4432e2169c3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-23T07:43:17.357921Z",
     "iopub.status.busy": "2024-07-23T07:43:17.357539Z",
     "iopub.status.idle": "2024-07-23T07:43:21.989910Z",
     "shell.execute_reply": "2024-07-23T07:43:21.988938Z",
     "shell.execute_reply.started": "2024-07-23T07:43:17.357891Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def detectText(img_path):\n",
    "    with io.open(img_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    \n",
    "    # Set the image context for handwritten Sanskrit\n",
    "    image = vision_v1.types.Image(content=content)\n",
    "    # image_context = vision_v1.types.ImageContext(language_hints=['sa'])  # 'sa' is the language code for Sanskrit\n",
    "    \n",
    "    # Perform text detection with image context\n",
    "    response = client.document_text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "    \n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame(columns=['locale', 'description'])\n",
    "    \n",
    "    # Collect data into a list of dictionaries\n",
    "    data = []\n",
    "    for text in texts:\n",
    "        data.append({\n",
    "            'locale': text.locale,\n",
    "            'description': text.description\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    new_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Append the new_df to the existing df using pd.concat\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return texts, df\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    all_texts_df = pd.DataFrame(columns=['locale', 'description', 'filename'])\n",
    "    \n",
    "    # Iterate over all files in the folder\n",
    "    for filename in tqdm(os.listdir(folder_path), desc=\"Processing images\"):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            # Perform OCR on the image\n",
    "            texts, df = detectText(image_path)\n",
    "            df['filename'] = filename  # Add filename to the DataFrame\n",
    "            \n",
    "            # Append to the overall DataFrame\n",
    "            all_texts_df = pd.concat([all_texts_df, df], ignore_index=True)\n",
    "            \n",
    "            # Display filename\n",
    "            print(f\"Filename: {filename}\")\n",
    "            \n",
    "            # Display the image with bounding boxes\n",
    "            visualize_ocr(image_path, texts)\n",
    "            \n",
    "            # Print detected text\n",
    "            print(\"Detected Text:\")\n",
    "            for text in texts:\n",
    "                print(text.description)\n",
    "    \n",
    "    return all_texts_df\n",
    "\n",
    "def visualize_ocr(image_path, texts):\n",
    "    image = cv2.imread(image_path)\n",
    "    for text in texts:\n",
    "        vertices = [(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]\n",
    "        pts = np.array(vertices, np.int32)\n",
    "        pts = pts.reshape((-1, 1, 2))\n",
    "        cv2.polylines(image, [pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "    \n",
    "    # Convert image from BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the image\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Folder containing the images\n",
    "FOLDER_PATH = \"/kaggle/input/sans-samplev2\"\n",
    "\n",
    "# Process the folder and get all texts\n",
    "all_texts_df = process_folder(FOLDER_PATH)\n",
    "\n",
    "# Print the DataFrame with all detected texts\n",
    "print(all_texts_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5375813,
     "sourceId": 8935542,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5427496,
     "sourceId": 9008783,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5431840,
     "sourceId": 9014894,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5431852,
     "sourceId": 9014945,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5427465,
     "sourceId": 9015346,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
